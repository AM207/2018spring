{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Risk\n",
    "\n",
    "##### Keywords: classification, supervised learning, decision risk, decision theory, bayes risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"poster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The multiple risks\n",
    "\n",
    "There are *two risks in learning* that we must consider, one to *estimate probabilities*, which we call **estimation risk**, and one to *make decisions*, which we call **decision risk**. \n",
    "\n",
    "What do we mean by a \"decision\" exactly? We'll use the letter $a$ here to indicate a decision, in both the regression and classification problems. In the classification problem, one example of a decision is the process used to choose the class of a sample, given the probability of being in that class.  We must mix these probabilities with \"business knowledge\" or \"domain knowledge\" to make a decision. \n",
    "\n",
    "What we must additionally supply is the **decision loss** $l(y,a)$ or **utility** $u(l,a)$ (profit, or benefit) in making a decision $a$ when the predicted variable has value $y$. For example, we must provide all of the losses $l$(no-cancer, biopsy), $l$(cancer, biopsy), $l$(no-cancer, no-biopsy), and $l$(cancer, no-biopsy). One set of choices for these losses may be 20, 0, 0, 200 respectively.\n",
    "\n",
    "To simplify matters though, lets presently insist that the **decision space** from which the decision $a$ is chosen is the same as the space from which $y$ is chosen. In other words, the decision to be made is a classification. Then we can use these losses to penalize mis-classification asymmetrically if we desire. In the cancer example, we then set $l$(observed-no-cancer, predicted cancer) to be 20 and $l$(observed-cancer, predicted-no-cancer) to be 200. This is the situation we talked about much earlier in the class where we penalize the false negative(observed cancer not predicted to be cancer) much more than the false positive(observed non-cancer predicted to be cancer). These estimates were obtained from the confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictive averaged risk for classification\n",
    "\n",
    " We simply weigh each combinations loss by the predictive probability that that combination can happen, the integral from the risks notes reducing to a sum:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "$$ R_{a}(x) = \\sum_y l(y,a(x)) p(y|x)$$\n",
    "\n",
    "That is, we calculate the **average risk** over all choices y, of making choice a for a given data point.\n",
    "\n",
    "Then, if we want to calculate the overall risk, given all the samples in our set, we calculate:\n",
    "\n",
    "$$R(a) = \\int dx p(x) R_{a}(x)$$\n",
    "\n",
    "(Since we usually assume fixed but unknown-distributed covariates, we can replace this integral by a sum over the empirical distribution, ie a sum over the data points)\n",
    "\n",
    "It is sufficient to minimize the risk at each point or sample to minimize the overall risk since $p(x)$ is always positive.\n",
    "\n",
    "Consider the two class classification case. Say we make a \"decision a about which class\" at a sample x. Then:\n",
    "\n",
    "$$R_a(x) = l(1, g)p(1|x) + l(0, g)p(0|x).$$\n",
    "\n",
    "Then for the \"decision\" $a=1$ we have:\n",
    "\n",
    "$$R_1(x) = l(1,1)p(1|x) + l(0,1)p(0|x),$$\n",
    "\n",
    "and for the \"decision\" $a=0$ we have:\n",
    "\n",
    "$$R_0(x) = l(1,0)p(1|x) + l(0,0)p(0|x).$$\n",
    "\n",
    "Now, we'd choose $1$ for the sample at $x$ if:\n",
    "\n",
    "$$R_1(x) \\lt R_0(x).$$\n",
    "\n",
    "$$ P(1|x)(l(1,1) - l(1,0)) \\lt p(0|x)(l(0,0) - l(0,1))$$\n",
    "\n",
    "This gives us a ratio `r` between the probabilities to make a prediction. We assume this is true for all samples.\n",
    "\n",
    "So, to choose '1', the Bayes risk can be obtained by setting:\n",
    "\n",
    "$$p(1|x) \\gt r P(0|x) \\implies r=\\frac{l(0,1) - l(0,0)}{l(1,0) - l(1,1)} =\\frac{c_{FP} - c_{TN}}{c_{FN} - c_{TP}}$$\n",
    "\n",
    "This may also be written as:\n",
    "\n",
    "$$P(1|x) \\gt t = \\frac{r}{1+r}$$.\n",
    "\n",
    "If you assume that True positives and True negatives have no cost, and the cost of a false positive is equal to that of a false positive, then $r=1$ and the threshold is the usual intuitive $t=0.5$.\n",
    "\n",
    "### The symmetric case with 1-0 risk\n",
    "\n",
    "For the 1-0 loss, $l(1,1) = l(0,0) =0$ and $l(1,0) = l(0,1) = 1$, and we get:\n",
    "\n",
    "$$R_1(x) = p(0|x), R_0(x) = p(1|x).$$\n",
    "\n",
    "We'd choose $1$ if:\n",
    "\n",
    "$$R_1(x) \\le R_0(x)$$\n",
    "\n",
    "for a given sample $x$. Thus we get back the \"intuitive\" prescrription for classification we have been using so far: **choose $1$ if**:\n",
    "\n",
    "$$p(1|x) \\ge p(0|x).$$\n",
    "\n",
    "Since these add to 1, this is equivalent to saying **choose $1$ if $p(1|x) \\ge 0.5$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
