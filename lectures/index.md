---
title: "Lectures"
shorttitle: Lectures
layout: "default"
---

- [Lecture 1](lecture1.html): Intro and Probability
- [Lecture 2](lecture2.html): Probability, Distributions, and Frequentism
- [lab 1](lab1.html): Frequentism
- [Lecture 3](lecture3.html): Expectations, the laws, and Monte Carlo
- [Lecture 4](lecture4.html): Sampling
- [lab 2](lab2.html): Python, Math, and Stratification
- [Lecture 5](lecture5.html): Machine Learning
- [Lecture 6](lecture6.html): Gradient Descent
- [lab 3](lab3.html): PyTorch, Regressions, and Artificial Neural Networks
- [Lecture 7](lecture7.html): Machine Learning and Backpropagation
- [Lecture 8](lecture8.html): Neural Nets and Information Theory
- [lab 4](lab4.html): PyTorch and Artificial Neural Networks(contd)
- [Lecture 9](lecture9.html): Information Theory, Deviance, and Global Optimization
- [Lecture 10](lecture10.html): Annealing, Markov, and Metropolis
- [lab 5](lab5.html): Simulated Annealing
- [Lecture 11](lecture11.html): Metropolis To Bayes
- [Lecture 12](lecture12.html): Bayes
- [lab 6](lab6.html): Sampling and Bayes
- [Lecture 13](lecture13.html): Bayes
- [Lecture 14](lecture14.html): Convergence and Gibbs
- [lab 7](lab7.html): Sampling and Hierarchical Models
- [Lecture 15](lecture15.html): Linear Regression
- [Lecture 16](lecture16.html): Gaussian Processes
